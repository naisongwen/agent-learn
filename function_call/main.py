import sys
import os
import logging
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from llm.client import LLMClient
from utils.logger import setup_logging

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logger = setup_logging(log_level="INFO", log_file="app")

def main():
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    client = LLMClient(model="gpt-4-turbo")
    
    # æ¼”ç¤ºåœºæ™¯1ï¼šå¤©æ°”æŸ¥è¯¢
    print("\nğŸ“ åœºæ™¯1ï¼šå¤©æ°”æŸ¥è¯¢")
    print("-" * 40)
    messages = [{"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]
    result = client.chat(messages)
    
    if result["success"]:
        print(f"AIå›ç­”ï¼š{result['content']}")
    else:
        print(f"é”™è¯¯ï¼š{result.get('error')}")
    # æ¼”ç¤ºåœºæ™¯2ï¼šè®¡ç®—
    print("\nğŸ”¢ åœºæ™¯2ï¼šæ•°å­¦è®¡ç®—")
    print("-" * 40)
    messages = [{"role": "user", "content": "å¸®æˆ‘è®¡ç®— (123+456)*789 ç­‰äºå¤šå°‘"}]
    result = client.chat(messages)
    
    if result["success"]:
        print(f"AIå›ç­”ï¼š{result['content']}")
    
    # æ¼”ç¤ºåœºæ™¯3ï¼šå¤šå·¥å…·åä½œ
    print("\nğŸ”„ åœºæ™¯3ï¼šå¤šå·¥å…·åä½œ")
    print("-" * 40)
    messages = [{
        "role": "user", 
        "content": "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿå¦‚æœåŒ—äº¬æ˜¯ä¸‹åˆï¼Œå¸®æˆ‘æŸ¥ä¸€ä¸‹å¤©æ°”"
    }]
    result = client.chat(messages)
    
    if result["success"]:
        print(f"AIå›ç­”ï¼š{result['content']}")
        print(f"å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼š{result.get('tool_calls_count', 0)}")
    
    # æ¼”ç¤ºåœºæ™¯4ï¼šç®€å•å¯¹è¯
    print("\nğŸ’¬ åœºæ™¯4ï¼šç®€å•å¯¹è¯ï¼ˆæ— å·¥å…·ï¼‰")
    print("-" * 40)
    response = client.chat_simple("è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    print(f"AIå›ç­”ï¼š{response}")
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
